{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIO DEL ANÁLISIS Y EXPORTACIÓN ---\n",
      "Conexión con Ollama (http://ollama:11434) exitosa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando 3409 personas:   0%|          | 15/3409 [05:40<15:12:20, 16.13s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuración ---\n",
    "JSON_FILE_PATH = '/scripts/LLM_Qdrant/Data/resultado_json.json'\n",
    "CSV_OUTPUT_PATH = '/scripts/LLM_Qdrant/Data/analisis_grupos_tematicos.csv'\n",
    "OLLAMA_HOST = 'http://ollama:11434'\n",
    "\n",
    "# --- PARÁMETROS PARA EL CLUSTERING (AJUSTABLES) ---\n",
    "DBSCAN_EPS = 0.4 \n",
    "DBSCAN_MIN_SAMPLES = 2\n",
    "\n",
    "# --- Modelos Sugeridos ---\n",
    "EMBEDDING_MODEL_NAME = 'snowflake-arctic-embed2:latest'\n",
    "GENERATIVE_MODEL_NAME = 'gemma3:latest' \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Función para limpiar el texto de saltos de línea y espacios extra.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ' '.join(text.replace('\\r\\n', ' ').replace('\\n', ' ').strip().split())\n",
    "\n",
    "def generate_cluster_summary(observations, client, model_name):\n",
    "    \"\"\"Usa un modelo generativo para crear un resumen temático de un grupo.\"\"\"\n",
    "    observations_list_str = \"\\n\".join([f\"- \\\"{obs}\\\"\" for obs in observations])\n",
    "    prompt = f\"\"\"\n",
    "    Actúa como un analista policial experto.\n",
    "    Tu tarea es leer la siguiente lista de observaciones y sintetizar el tema central o el evento principal que las une en una sola frase concisa y clara.\n",
    "    No repitas frases completas. Enfócate en el núcleo del asunto.\n",
    "\n",
    "    Observaciones:\n",
    "    {observations_list_str}\n",
    "\n",
    "    Tema central del grupo (en una sola frase concisa en español):\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat(\n",
    "            model=model_name,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            options={'temperature': 0.1}\n",
    "        )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"  [!] Advertencia: No se pudo generar el resumen. Error: {e}\")\n",
    "        return \"Error al generar resumen\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal para procesar personas, agrupar sus observaciones,\n",
    "    generar resúmenes y guardar todo en un CSV con barra de progreso.\n",
    "    \"\"\"\n",
    "    print(\"--- INICIO DEL ANÁLISIS Y EXPORTACIÓN ---\")\n",
    "    \n",
    "    # --- 1. Conexión a Ollama y carga de datos ---\n",
    "    try:\n",
    "        client = ollama.Client(host=OLLAMA_HOST)\n",
    "        client.list() \n",
    "        print(f\"Conexión con Ollama ({OLLAMA_HOST}) exitosa.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: No se pudo conectar con Ollama. Detalle: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: El archivo no se encontró en la ruta: {JSON_FILE_PATH}\")\n",
    "        return\n",
    "\n",
    "    person_ids = list(data.keys())\n",
    "    total_observations_processed = 0\n",
    "\n",
    "    # --- 2. Preparación y escritura en CSV ---\n",
    "    try:\n",
    "        with open(CSV_OUTPUT_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            # Escribimos la cabecera del CSV\n",
    "            csv_writer.writerow([\n",
    "                'id_persona', \n",
    "                'id_grupo_tematico', \n",
    "                'resumen_del_grupo', \n",
    "                'id_observacion_original', \n",
    "                'texto_observacion'\n",
    "            ])\n",
    "\n",
    "            # Usamos tqdm para la barra de progreso, iterando sobre cada persona\n",
    "            for person_id in tqdm(person_ids, desc=f\"Procesando {len(person_ids)} personas\"):\n",
    "                content = data[person_id]\n",
    "                \n",
    "                # --- 3. Recolección de observaciones para la persona actual ---\n",
    "                all_observations_with_meta = []\n",
    "                for key, value in content.items():\n",
    "                    if key.startswith(\"observacion_\") and isinstance(value, list):\n",
    "                        obs_type = key.replace(\"observacion_\", \"\").upper()\n",
    "                        for index, obs_text in enumerate(value):\n",
    "                            cleaned_obs = clean_text(obs_text)\n",
    "                            if cleaned_obs:\n",
    "                                all_observations_with_meta.append({\n",
    "                                    \"text\": cleaned_obs,\n",
    "                                    \"type\": obs_type,\n",
    "                                    \"original_index\": index + 1,\n",
    "                                    \"composite_id\": f\"{person_id}-{obs_type}-{index + 1}\"\n",
    "                                })\n",
    "\n",
    "                # Si no hay suficientes observaciones para comparar, se guardan como \"sin grupo\"\n",
    "                if len(all_observations_with_meta) < DBSCAN_MIN_SAMPLES:\n",
    "                    rows_to_write = []\n",
    "                    for meta_info in all_observations_with_meta:\n",
    "                        rows_to_write.append([\n",
    "                            person_id,\n",
    "                            f\"{person_id}-SIN_GRUPO\",\n",
    "                            \"Observación aislada (no se pudo agrupar)\",\n",
    "                            meta_info['composite_id'],\n",
    "                            meta_info['text']\n",
    "                        ])\n",
    "                    if rows_to_write:\n",
    "                        csv_writer.writerows(rows_to_write)\n",
    "                        total_observations_processed += len(rows_to_write)\n",
    "                    continue\n",
    "\n",
    "                # --- 4. Generación de Embeddings y Clustering ---\n",
    "                observations_texts = [obs['text'] for obs in all_observations_with_meta]\n",
    "                try:\n",
    "                    embeddings = [\n",
    "                        res['embedding'] for res in (\n",
    "                            client.embeddings(model=EMBEDDING_MODEL_NAME, prompt=obs) for obs in observations_texts\n",
    "                        )\n",
    "                    ]\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError al generar embeddings para Persona ID: {person_id}. Detalle: {e}. Saltando...\")\n",
    "                    continue\n",
    "\n",
    "                clustering = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES, metric='cosine').fit(np.array(embeddings))\n",
    "                labels = clustering.labels_\n",
    "                \n",
    "                # --- 5. Procesamiento de resultados y preparación para CSV ---\n",
    "                rows_to_write = []\n",
    "                unique_labels = sorted(set(labels))\n",
    "\n",
    "                for cluster_id in unique_labels:\n",
    "                    cluster_indices = np.where(labels == cluster_id)[0]\n",
    "                    \n",
    "                    if cluster_id == -1:\n",
    "                        # Observaciones que no pertenecen a ningún grupo (ruido)\n",
    "                        summary = \"Observación sin grupo temático (ruido)\"\n",
    "                        group_id_str = f\"{person_id}-SIN_GRUPO\"\n",
    "                    else:\n",
    "                        # Observaciones que sí pertenecen a un grupo\n",
    "                        observations_in_cluster = [observations_texts[i] for i in cluster_indices]\n",
    "                        summary = generate_cluster_summary(observations_in_cluster, client, GENERATIVE_MODEL_NAME)\n",
    "                        group_id_str = f\"{person_id}-GRUPO-{cluster_id + 1}\"\n",
    "\n",
    "                    # Crear una fila en el CSV para cada observación en este grupo (o ruido)\n",
    "                    for i in cluster_indices:\n",
    "                        meta_info = all_observations_with_meta[i]\n",
    "                        rows_to_write.append([\n",
    "                            person_id,\n",
    "                            group_id_str,\n",
    "                            summary,\n",
    "                            meta_info['composite_id'],\n",
    "                            meta_info['text']\n",
    "                        ])\n",
    "                \n",
    "                if rows_to_write:\n",
    "                    csv_writer.writerows(rows_to_write)\n",
    "                    total_observations_processed += len(rows_to_write)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nHa ocurrido un error inesperado durante el proceso: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"--- PROCESO COMPLETADO ---\")\n",
    "    print(f\"[✓] Se han analizado y guardado {total_observations_processed} observaciones de {len(person_ids)} personas.\")\n",
    "    print(f\"Los resultados se han guardado en: {CSV_OUTPUT_PATH}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
